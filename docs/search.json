[
  {
    "objectID": "day-09-10.html",
    "href": "day-09-10.html",
    "title": "Daily Exercise 09-10",
    "section": "",
    "text": "Using the airquality from R datasets\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(visdat)\ndata(\"airquality\")\n\n\nUse the help (?) function to learn more about the dataset\n\n\n?airquality\n\n\nUse vis_dat to check out the data. Does it need cleaning?\n\nYes, the data needs cleaning because there are NA values in Ozone and Solar.R.\n\nvis_dat(airquality)\n\n\n\n\n\n\n\n# cleaning ozone and solar data\nairquality &lt;- airquality %&gt;%\n  drop_na()\nvis_dat(airquality)\n\n\n\n\n\n\n\n\n\nFit a linear model to the cleaned data to predict Ozone from one of the possible predictors of your choosing. Why did you chose that variable?\n\nTemperature is a possible predictor for ozone because it has the strongest correlation with ozone at 0.29.\n\ncor(airquality[, c(\"Ozone\", \"Solar.R\", \"Wind\", \"Temp\")], use=\"complete.obs\")\n\n             Ozone    Solar.R       Wind       Temp\nOzone    1.0000000  0.3483417 -0.6124966  0.6985414\nSolar.R  0.3483417  1.0000000 -0.1271835  0.2940876\nWind    -0.6124966 -0.1271835  1.0000000 -0.4971897\nTemp     0.6985414  0.2940876 -0.4971897  1.0000000\n\nlm_model &lt;- lm(Ozone ~ Temp, data = airquality)\n\n\nUsing summary(), Does this seem like a valid model?\n\nYes, the linear model of ozone and temperature seems like a valid model.\n\nsummary(lm_model) \n\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.922 -17.459  -0.874  10.444 118.078 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -147.6461    18.7553  -7.872 2.76e-12 ***\nTemp           2.4391     0.2393  10.192  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.92 on 109 degrees of freedom\nMultiple R-squared:  0.488, Adjusted R-squared:  0.4833 \nF-statistic: 103.9 on 1 and 109 DF,  p-value: &lt; 2.2e-16\n\n\n\nExplain the R2 found in a sentence.\n\n48.8% of the variation in ozone levels can be explained by temperature, while the remaining 51.2% is due to other factors not included in the model.\n\nUse broom::augment to predict the Ozone of the cleaned data\n\n\nlibrary(broom)\npredicted_data &lt;- augment(lm_model, data=airquality)\nhead(predicted_data)\n\n# A tibble: 6 × 12\n  Ozone Solar.R  Wind  Temp Month   Day .fitted .resid   .hat .sigma  .cooksd\n  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1    41     190   7.4    67     5     1   15.8   25.2  0.0207   23.9 0.0120  \n2    36     118   8      72     5     2   28.0    8.03 0.0124   24.0 0.000714\n3    12     149  12.6    74     5     3   32.8  -20.8  0.0104   23.9 0.00405 \n4    18     313  11.5    62     5     4    3.58  14.4  0.0340   24.0 0.00662 \n5    23     299   8.6    65     5     7   10.9   12.1  0.0254   24.0 0.00342 \n6    19      99  13.8    59     5     8   -3.74  22.7  0.0444   23.9 0.0219  \n# ℹ 1 more variable: .std.resid &lt;dbl&gt;\n\n\n\nUse ggplot to plot the actual vs predicted Ozone\n\n\nAdd a red line to show where the actual and predicted values are equal This can be done by plotting a 1:1 line (e.g. intercept 0, slope 1) with geom_abline(intercept = 0, slope = 1, color = “red”)\nAdd a subtitle to the plot showing the correlation between the actual and predicted values are equal This can be done by plotting a 1:1 line (e.g. intercept 0, slope 1) with paste(“Correlation:”, round(cor(a\\(Ozone, a\\).fitted),2)) assuming your augmented data object is called ‘a’\n\n\nggplot(predicted_data, aes(x = Ozone, y = .fitted)) +\n  geom_point(alpha = 0.7, color = \"blue\") +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") + \n  labs(title = \"Actual vs Predicted Ozone Levels\",\n       x = \"Actual Ozone\",\n       y = \"Predicted Ozone\",\n       subtitle = paste(\"Correlation:\", round(cor(predicted_data$Ozone, predicted_data$.fitted), 2))) +\n  theme_minimal()"
  },
  {
    "objectID": "day11-12.html",
    "href": "day11-12.html",
    "title": "Daily Exercise 11/12",
    "section": "",
    "text": "In this assignment, you will again analyze the airquality dataset using various statistical tests, data transformation techniques, and regression modeling. Follow the step-by-step guiding questions to complete the analysis in a qmd.\n\n\n\nLoad the airquality dataset in R. What does this dataset represent? Explore its structure using functions like str() and summary().\n\nThe airquality dataset contains daily air pollution measurements collected in New York, USA, during May to September 1973. It is a data frame with 153 observations on 6 variables: ozone, solar.r, wind, temp, month, and day, which are all numeric variables.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ recipes      1.1.0     ✔ yardstick    1.3.1\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\ndata(\"airquality\")\nstr(airquality)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n\n\nPerform a Shapiro-Wilk normality test on the following variables: Ozone, Temp, Solar.R, and Wind.\n\n\nshapiro.test(airquality$Ozone)\n\n\n    Shapiro-Wilk normality test\n\ndata:  airquality$Ozone\nW = 0.87867, p-value = 2.79e-08\n\nshapiro.test(airquality$Temp)\n\n\n    Shapiro-Wilk normality test\n\ndata:  airquality$Temp\nW = 0.97617, p-value = 0.009319\n\nshapiro.test(airquality$Solar.R)\n\n\n    Shapiro-Wilk normality test\n\ndata:  airquality$Solar.R\nW = 0.94183, p-value = 9.492e-06\n\nshapiro.test(airquality$Wind)\n\n\n    Shapiro-Wilk normality test\n\ndata:  airquality$Wind\nW = 0.98575, p-value = 0.1178\n\n\n\nWhat is the purpose of the Shapiro-Wilk test?\n\nThe purpose of the Shapiro-Wilk test is to statistically check if the given dataset follows a normal distribution. For example, if the p-value &gt; 0.05, then the data is normal. However, if the p-value &lt; 0.05, then the data is not normally distrubuted.\n\nWhat are the null and alternative hypotheses for this test?\n\nThe null hypothesis for each variable that was tested, for example, Ozone, is that Temp, Solar.R, and Wind has no effect on Ozone. The alternative hypotheses for this test (ex. Ozone) is that at least one of the other variables has an effect on Ozone.\n\nInterpret the p-values. Are these variables normally distributed?\n\nNo, only Wind is normally distributed (0.1178 &gt; 0.05). Ozone, Temp, and Solar.R all has p-values less than 0.05, indicating that these variables are not normally distributed.\n\n\n\n\nCreate a new column with case_when translating the Months into four seasons (Winter (Nov, Dec, Jan), Spring (Feb, Mar, Apr), Summer (May, Jun, Jul), and Fall (Aug, Sep, Oct)).\n\n\nairquality &lt;- airquality %&gt;%\n  mutate(Season = case_when(\n    Month %in% c(11, 12, 1) ~ \"Winter\",\n    Month %in% c(2, 3, 4) ~ \"Spring\",\n    Month %in% c(5, 6, 7) ~ \"Summer\",\n    Month %in% c(8, 9, 10) ~ \"Fall\"\n  ))\n\n\nUse table to figure out how many observations we have from each season.\n\n\ntable(airquality$Season)\n\n\n  Fall Summer \n    61     92 \n\n\n\n\n\n\nNormalize the predictor variables (Temp, Solar.R, Wind, and Season) using a recipe\n\n\nairquality &lt;- airquality %&gt;%\n  mutate(across(c(Ozone, Solar.R, Wind, Temp), ~ if_else(is.na(.), mean(., na.rm = TRUE), .)))\nrecipe_obj &lt;- recipe(Ozone ~ Temp + Solar.R + Wind + Season, data = airquality) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n\nWhat is the purpose of normalizing data?\n\nNormalizing data puts all of the numeric variables on the same scale, which prevents bias in models, preventing one variable from dominating others.\n\nWhat function can be used to impute missing values with the mean?\n\nThree functions that can be used to impute missing values with the mean is an if_else, replace_na, or step_impute_mean.\n\nprep and bake the data to generate a processed dataset.\n\n\nprep_recipe &lt;- prep(recipe_obj, training = airquality)\nnormalized_data &lt;- bake(prep_recipe, new_data = NULL)\n\nhead(normalized_data)\n\n# A tibble: 6 × 5\n    Temp   Solar.R   Wind Ozone Season_Summer\n   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 -1.15   4.63e- 2 -0.726  41           0.812\n2 -0.621 -7.72e- 1 -0.556  36           0.812\n3 -0.410 -4.20e- 1  0.750  12           0.812\n4 -1.68   1.44e+ 0  0.438  18           0.812\n5 -2.31  -3.23e-16  1.23   42.1         0.812\n6 -1.26  -3.23e-16  1.40   28           0.812\n\nnormalized_data &lt;- normalized_data %&gt;%\n  rename(Season = Season_Summer)\n\n\nWhy is it necessary to both prep() and bake() the recipe?\n\nIt is necessary to both prep() and bake() the recipe because after the recipe determined variables to reprocess before modeling, prep() estimates the parameters and bake() applies the transformations to the dataset.\n\n\n\n\nFit a linear model using Ozone as the response variable and all other variables as predictors. Remember that the . notation can we used to include all variables.\n\n\nmodel &lt;- lm(Ozone ~ ., data = normalized_data)\nglance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.481         0.467  20.9      34.3 2.99e-20     4  -680. 1372. 1390.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nlibrary(ggpubr)\npred &lt;- augment(model, normalized_data)\npred &lt;- pred %&gt;%\n  mutate(Temp_Category = cut(Temp, breaks = 3, labels = c(\"Low\", \"Medium\", \"High\")))\nggscatter(pred,\n          x = 'Ozone', y = '.fitted',\n          add = \"reg.line\", conf.int = TRUE,\n          cor.coef = TRUE, cor.method = \"pearson\",\n          color = \"Temp_Category\", palette = \"jco\")\n\n\n\n\n\n\n\n# i could not get all of the variables\n\n\nInterpret the model summary output (coefficients, R-squared, p-values) in plain language\n\nSince the R^2 is 60.94%, this indicated this ozone does a reasonable good job at predicting ozone levels. The p-value is less than 0.05, meaning that the model is statistically significant, where at least one predictor explains variation in ozone levels. The F-statistic is 41.34, which indicates strong evidence that the predictors improve the model.\n\n\n\n\nUse broom::augment to suppliment the normalized data.frame with the fitted values and residuals.\n\n\nmodel_diagnostics &lt;- augment(model, data = normalized_data)\nhead(model_diagnostics)\n\n# A tibble: 6 × 11\n    Temp   Solar.R   Wind Ozone Season .fitted .resid   .hat .sigma   .cooksd\n   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 -1.15   4.63e- 2 -0.726  41    0.812   36.3    4.72 0.0313   21.0 0.000339 \n2 -0.621 -7.72e- 1 -0.556  36    0.812   37.0   -1.04 0.0217   21.0 0.0000111\n3 -0.410 -4.20e- 1  0.750  12    0.812   28.8  -16.8  0.0157   21.0 0.00209  \n4 -1.68   1.44e+ 0  0.438  18    0.812   25.6   -7.58 0.0520   21.0 0.00151  \n5 -2.31  -3.23e-16  1.23   42.1  0.812    3.33  38.8  0.0446   20.8 0.0336   \n6 -1.26  -3.23e-16  1.40   28    0.812   14.4   13.6  0.0247   21.0 0.00218  \n# ℹ 1 more variable: .std.resid &lt;dbl&gt;\n\n\n\nExtract the residuals and visualize their distribution as a histogram and qqplot.\n\n\nhist &lt;- gghistogram(model_diagnostics, x = \".resid\",\n                    bins = 30, fill = \"blue\", color = \"white\", title = \"Histogram of Residuals\")\nqq_plot &lt;- ggqqplot(model_diagnostics, x = \".resid\",\n                   title = \"QQ Plot of Residuals\")\n\n\nUse ggarange to plot this as one image and interpret what you see in them.\n\nIn the histogram, the residuals appear normally distributed but slightly skewed. The concentration around zero suggests the model captures the variation in ozone levels, aside from some outliers. In the qq plot, the residuals mostly follow the 45 degree line, however, there is some slight deviation of the tail, suggesting that the residuals are not perfectly normal.\n\nggarrange(hist,qq_plot, ncol = 2)\n\n\n\n\n\n\n\n\n\nCreate a scatter plot of actual vs. predicted values using ggpubr with the following setting:\n\n\nggscatter(model_diagnostics, x = \"Ozone\", y = \".fitted\",\n          add = \"reg.line\", conf.int = TRUE,\n          cor.coef = TRUE, cor.method = \"spearman\",\n          ellipse = TRUE)\n\n\n\n\n\n\n\n\n\nHow strong of a model do you think this is?\n\nThe scatterplot shows a strong correlation (R=0.77 and p &lt; 2.2e-16), indicating that the model is strong and shows the relationship between predicted and actual Ozone values. The linear line and ellipse around the central data suggests that the model is a good fit aside from some outliers."
  },
  {
    "objectID": "day11-12.html#exploratory-data-analysis-and-linear-regression-in-r",
    "href": "day11-12.html#exploratory-data-analysis-and-linear-regression-in-r",
    "title": "Daily Exercise 11/12",
    "section": "",
    "text": "In this assignment, you will again analyze the airquality dataset using various statistical tests, data transformation techniques, and regression modeling. Follow the step-by-step guiding questions to complete the analysis in a qmd.\n\n\n\nLoad the airquality dataset in R. What does this dataset represent? Explore its structure using functions like str() and summary().\n\nThe airquality dataset contains daily air pollution measurements collected in New York, USA, during May to September 1973. It is a data frame with 153 observations on 6 variables: ozone, solar.r, wind, temp, month, and day, which are all numeric variables.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ recipes      1.1.0     ✔ yardstick    1.3.1\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\ndata(\"airquality\")\nstr(airquality)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n\n\nPerform a Shapiro-Wilk normality test on the following variables: Ozone, Temp, Solar.R, and Wind.\n\n\nshapiro.test(airquality$Ozone)\n\n\n    Shapiro-Wilk normality test\n\ndata:  airquality$Ozone\nW = 0.87867, p-value = 2.79e-08\n\nshapiro.test(airquality$Temp)\n\n\n    Shapiro-Wilk normality test\n\ndata:  airquality$Temp\nW = 0.97617, p-value = 0.009319\n\nshapiro.test(airquality$Solar.R)\n\n\n    Shapiro-Wilk normality test\n\ndata:  airquality$Solar.R\nW = 0.94183, p-value = 9.492e-06\n\nshapiro.test(airquality$Wind)\n\n\n    Shapiro-Wilk normality test\n\ndata:  airquality$Wind\nW = 0.98575, p-value = 0.1178\n\n\n\nWhat is the purpose of the Shapiro-Wilk test?\n\nThe purpose of the Shapiro-Wilk test is to statistically check if the given dataset follows a normal distribution. For example, if the p-value &gt; 0.05, then the data is normal. However, if the p-value &lt; 0.05, then the data is not normally distrubuted.\n\nWhat are the null and alternative hypotheses for this test?\n\nThe null hypothesis for each variable that was tested, for example, Ozone, is that Temp, Solar.R, and Wind has no effect on Ozone. The alternative hypotheses for this test (ex. Ozone) is that at least one of the other variables has an effect on Ozone.\n\nInterpret the p-values. Are these variables normally distributed?\n\nNo, only Wind is normally distributed (0.1178 &gt; 0.05). Ozone, Temp, and Solar.R all has p-values less than 0.05, indicating that these variables are not normally distributed.\n\n\n\n\nCreate a new column with case_when translating the Months into four seasons (Winter (Nov, Dec, Jan), Spring (Feb, Mar, Apr), Summer (May, Jun, Jul), and Fall (Aug, Sep, Oct)).\n\n\nairquality &lt;- airquality %&gt;%\n  mutate(Season = case_when(\n    Month %in% c(11, 12, 1) ~ \"Winter\",\n    Month %in% c(2, 3, 4) ~ \"Spring\",\n    Month %in% c(5, 6, 7) ~ \"Summer\",\n    Month %in% c(8, 9, 10) ~ \"Fall\"\n  ))\n\n\nUse table to figure out how many observations we have from each season.\n\n\ntable(airquality$Season)\n\n\n  Fall Summer \n    61     92 \n\n\n\n\n\n\nNormalize the predictor variables (Temp, Solar.R, Wind, and Season) using a recipe\n\n\nairquality &lt;- airquality %&gt;%\n  mutate(across(c(Ozone, Solar.R, Wind, Temp), ~ if_else(is.na(.), mean(., na.rm = TRUE), .)))\nrecipe_obj &lt;- recipe(Ozone ~ Temp + Solar.R + Wind + Season, data = airquality) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n\nWhat is the purpose of normalizing data?\n\nNormalizing data puts all of the numeric variables on the same scale, which prevents bias in models, preventing one variable from dominating others.\n\nWhat function can be used to impute missing values with the mean?\n\nThree functions that can be used to impute missing values with the mean is an if_else, replace_na, or step_impute_mean.\n\nprep and bake the data to generate a processed dataset.\n\n\nprep_recipe &lt;- prep(recipe_obj, training = airquality)\nnormalized_data &lt;- bake(prep_recipe, new_data = NULL)\n\nhead(normalized_data)\n\n# A tibble: 6 × 5\n    Temp   Solar.R   Wind Ozone Season_Summer\n   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 -1.15   4.63e- 2 -0.726  41           0.812\n2 -0.621 -7.72e- 1 -0.556  36           0.812\n3 -0.410 -4.20e- 1  0.750  12           0.812\n4 -1.68   1.44e+ 0  0.438  18           0.812\n5 -2.31  -3.23e-16  1.23   42.1         0.812\n6 -1.26  -3.23e-16  1.40   28           0.812\n\nnormalized_data &lt;- normalized_data %&gt;%\n  rename(Season = Season_Summer)\n\n\nWhy is it necessary to both prep() and bake() the recipe?\n\nIt is necessary to both prep() and bake() the recipe because after the recipe determined variables to reprocess before modeling, prep() estimates the parameters and bake() applies the transformations to the dataset.\n\n\n\n\nFit a linear model using Ozone as the response variable and all other variables as predictors. Remember that the . notation can we used to include all variables.\n\n\nmodel &lt;- lm(Ozone ~ ., data = normalized_data)\nglance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.481         0.467  20.9      34.3 2.99e-20     4  -680. 1372. 1390.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nlibrary(ggpubr)\npred &lt;- augment(model, normalized_data)\npred &lt;- pred %&gt;%\n  mutate(Temp_Category = cut(Temp, breaks = 3, labels = c(\"Low\", \"Medium\", \"High\")))\nggscatter(pred,\n          x = 'Ozone', y = '.fitted',\n          add = \"reg.line\", conf.int = TRUE,\n          cor.coef = TRUE, cor.method = \"pearson\",\n          color = \"Temp_Category\", palette = \"jco\")\n\n\n\n\n\n\n\n# i could not get all of the variables\n\n\nInterpret the model summary output (coefficients, R-squared, p-values) in plain language\n\nSince the R^2 is 60.94%, this indicated this ozone does a reasonable good job at predicting ozone levels. The p-value is less than 0.05, meaning that the model is statistically significant, where at least one predictor explains variation in ozone levels. The F-statistic is 41.34, which indicates strong evidence that the predictors improve the model.\n\n\n\n\nUse broom::augment to suppliment the normalized data.frame with the fitted values and residuals.\n\n\nmodel_diagnostics &lt;- augment(model, data = normalized_data)\nhead(model_diagnostics)\n\n# A tibble: 6 × 11\n    Temp   Solar.R   Wind Ozone Season .fitted .resid   .hat .sigma   .cooksd\n   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 -1.15   4.63e- 2 -0.726  41    0.812   36.3    4.72 0.0313   21.0 0.000339 \n2 -0.621 -7.72e- 1 -0.556  36    0.812   37.0   -1.04 0.0217   21.0 0.0000111\n3 -0.410 -4.20e- 1  0.750  12    0.812   28.8  -16.8  0.0157   21.0 0.00209  \n4 -1.68   1.44e+ 0  0.438  18    0.812   25.6   -7.58 0.0520   21.0 0.00151  \n5 -2.31  -3.23e-16  1.23   42.1  0.812    3.33  38.8  0.0446   20.8 0.0336   \n6 -1.26  -3.23e-16  1.40   28    0.812   14.4   13.6  0.0247   21.0 0.00218  \n# ℹ 1 more variable: .std.resid &lt;dbl&gt;\n\n\n\nExtract the residuals and visualize their distribution as a histogram and qqplot.\n\n\nhist &lt;- gghistogram(model_diagnostics, x = \".resid\",\n                    bins = 30, fill = \"blue\", color = \"white\", title = \"Histogram of Residuals\")\nqq_plot &lt;- ggqqplot(model_diagnostics, x = \".resid\",\n                   title = \"QQ Plot of Residuals\")\n\n\nUse ggarange to plot this as one image and interpret what you see in them.\n\nIn the histogram, the residuals appear normally distributed but slightly skewed. The concentration around zero suggests the model captures the variation in ozone levels, aside from some outliers. In the qq plot, the residuals mostly follow the 45 degree line, however, there is some slight deviation of the tail, suggesting that the residuals are not perfectly normal.\n\nggarrange(hist,qq_plot, ncol = 2)\n\n\n\n\n\n\n\n\n\nCreate a scatter plot of actual vs. predicted values using ggpubr with the following setting:\n\n\nggscatter(model_diagnostics, x = \"Ozone\", y = \".fitted\",\n          add = \"reg.line\", conf.int = TRUE,\n          cor.coef = TRUE, cor.method = \"spearman\",\n          ellipse = TRUE)\n\n\n\n\n\n\n\n\n\nHow strong of a model do you think this is?\n\nThe scatterplot shows a strong correlation (R=0.77 and p &lt; 2.2e-16), indicating that the model is strong and shows the relationship between predicted and actual Ozone values. The linear line and ellipse around the central data suggests that the model is a good fit aside from some outliers."
  },
  {
    "objectID": "day11-12.html#part-3-data-preprocessing",
    "href": "day11-12.html#part-3-data-preprocessing",
    "title": "Daily Exercise 11/12",
    "section": "Part 3: Data Preprocessing",
    "text": "Part 3: Data Preprocessing\n\nNormalize the predictor variables (Temp, Solar.R, Wind, and Season) using a recipe\n\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\n\n\nWhat is the purpose of normalizing data?\nWhat function can be used to impute missing values with the mean?\nprep and bake the data to generate a processed dataset.\nWhy is it necessary to both prep() and bake() the recipe?\n\n\nPart 4: Building a Linear Regression Model\n\nFit a linear model using Ozone as the response variable and all other variables as predictors. Remeber that the . notation can we used to include all variables.\nInterpret the model summary output (coefficients, R-squared, p-values) in plain language\n\n\n\nPart 5: Model Diagnostics\n\nUse broom::augment to suppliment the normalized data.frame with the fitted values and residuals.\nExtract the residuals and visualize their distribution as a histogram and qqplot.\n\n\nlibrary(ggpubr)\n\n\nUse ggarange to plot this as one image and interpret what you see in them.\nCreate a scatter plot of actual vs. predicted values using ggpubr with the following setting:\n\nggscatter(a, x = “Ozone”, y = “.fitted”, add = “reg.line”, conf.int = TRUE, cor.coef = TRUE, cor.method = “spearman”, ellipse = TRUE)\n\nHow strong of a model do you think this is?"
  },
  {
    "objectID": "day-21.html",
    "href": "day-21.html",
    "title": "Daily Assignment 21",
    "section": "",
    "text": "library(dataRetrieval)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tsibble)\n\nRegistered S3 method overwritten by 'tsibble':\n  method               from \n  as_tibble.grouped_df dplyr\n\n\n\nAttaching package: 'tsibble'\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, union\n\n# Example: Cache la Poudre River at Mouth (USGS site 06752260)\npoudre_flow &lt;- readNWISdv(siteNumber = \"06752260\",    # Download data from USGS for site 06752260\n                          parameterCd = \"00060\",      # Parameter code 00060 = discharge in cfs)\n                          startDate = \"2013-01-01\",   # Set the start date\n                          endDate = \"2023-12-31\") |&gt;  # Set the end date\n  renameNWISColumns() |&gt;                              # Rename columns to standard names (e.g., \"Flow\", \"Date\")\n  mutate(Date = yearmonth(Date)) |&gt;                   # Convert daily Date values into a year-month format (e.g., \"2023 Jan\")\n  group_by(Date) |&gt;                                   # Group the data by the new monthly Date\n  summarise(Flow = mean(Flow))                       # Calculate the average daily flow for each month\n\n# 1. Convert to tsibble\npoudre_ts &lt;- poudre_flow %&gt;%\n  as_tsibble(index = Date)\n\npoudre_ts\n\n# A tsibble: 132 x 2 [1M]\n       Date    Flow\n      &lt;mth&gt;   &lt;dbl&gt;\n 1 2013 Jan   18.1 \n 2 2013 Feb   18.0 \n 3 2013 Mar    8.21\n 4 2013 Apr    5.94\n 5 2013 May  333.  \n 6 2013 Jun  300.  \n 7 2013 Jul   75.6 \n 8 2013 Aug   48.8 \n 9 2013 Sep 1085.  \n10 2013 Oct  146.  \n# ℹ 122 more rows\n\n# 2. Plotting the time series\nlibrary(ggplot2)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nflowplot &lt;- ggplot(poudre_ts, aes(x = Date, y = Flow)) +\n  geom_line() +\n  labs(title = \"Monthly Mean Discharge\\nCache la Poudre River (2013–2023)\",\n       x = \"Year‑Month\", y = \"Discharge (cfs)\")\n\nprint(flowplot)\n\n\n\n\n\n\n\nggplotly(flowplot)\n\n\n\n\n# 3. Subseries\nlibrary(feasts)\n\nLoading required package: fabletools\n\npoudre_ts %&gt;%\n  gg_subseries(Flow) +\n  labs(title = \"Subseries Plot of Monthly Flow\",\n       y = \"Discharge (cfs)\")\n\n\n\n\n\n\n\n\nIn this plot, the “seasons” are the calendar months (January to December), with each little line as the monthly totals plotted over the years. The subseries allows you to compare how the flows varied over the years from 2013-2023 by individual months. From the plot, it is seen that there are peaks in late spring/early summer and lows in winter, revealing the seasonal cycle of snow melt runoff.\n\n# 4. Decompose\nlibrary(fable)\nstl_mod &lt;- poudre_ts %&gt;%\n  model(\n    STL(Flow ~ trend(window = 13) + season(window = \"periodic\"))\n  )\ncomponents &lt;- stl_mod %&gt;% components()\n\nautoplot(components) +\n  labs(title = \"STL Decomposition of Monthly Flow\")\n\n\n\n\n\n\n\n\nIn this plot, the trend is a slowly varying line showing whether baseline flows are rising/falling over the decade. Seasonally, there is a repeating annual cycle, where it peaks in the spring, then troughs in the winter. I think the trend represents the basin’s baseline discharge, such as changes in the climate, land, or water-use policies. The seasonal pattern is driven by the hydrological cycle, where snow accumulates through the winter, then melts rapidly in late spring."
  }
]